{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d145818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219c9730",
   "metadata": {},
   "source": [
    "Data shares of the target datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fdd26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average data shares of the target for each div and p over runs\n",
    "for div in [\"10\", \"5\", \"0\"]:\n",
    "    print(f\"\\ndiv={div}:\")\n",
    "    for p in [\"5000\", \"12000\", \"25000\", \"50000\", \"100000\", \"all\"]:\n",
    "        dataframes = []\n",
    "        for run in [\"case1_run1\", \"case1_run2\", \"case1_run3\", \"case1_run4\", \"case1_run5\"]:\n",
    "            target = pd.read_parquet(f\"{config.path_to_data}/{run}/interim/target_div{div}_sim{p}{run}/target_div{div}_sim{p}{run}.parquet\")\n",
    "            shares = target.drop_duplicates(subset=\"id_ts\").groupby(by=\"data\").count()[\"id_ts\"]\n",
    "            dataframes.append(shares)\n",
    "        shares_over_runs = pd.concat(dataframes, axis=1)\n",
    "        mean_shares = shares_over_runs.mean(axis=1)\n",
    "        std_shares = shares_over_runs.std(axis=1)\n",
    "        \n",
    "        result = pd.DataFrame({\n",
    "            'data': mean_shares.index,\n",
    "            f'shares_p{p}': [f\"{mean_val:.2f} ({std_val:.2f})\" for mean_val, std_val in zip(mean_shares.values, std_shares.values)]\n",
    "        })\n",
    "        \n",
    "        \n",
    "        print(f\"\\np={p}:\")\n",
    "        print(result.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d87b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pie chart with average data shares over runs per p and for one div\n",
    "div = \"10\" #\"5\", \"0\"\n",
    "p_values = [\"5000\", \"12000\", \"25000\", \"50000\", \"100000\", \"all\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12), gridspec_kw={'hspace': 0.4, 'wspace': 0.3})\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, p in enumerate(p_values):\n",
    "    dataframes = []\n",
    "    for run in [\"case1_run1\", \"case1_run2\", \"case1_run3\", \"case1_run4\", \"case1_run5\"]:\n",
    "        target = pd.read_parquet(f\"{config.path_to_data}/{run}/interim/target_div{div}_sim{p}{run}/target_div{div}_sim{p}{run}.parquet\")\n",
    "        shares = target.drop_duplicates(subset=\"id_ts\").groupby(by=\"data\").count()[\"id_ts\"]\n",
    "        dataframes.append(shares)\n",
    "    \n",
    "    shares_over_runs = pd.concat(dataframes, axis=1)\n",
    "    mean_shares = shares_over_runs.mean(axis=1)\n",
    "    \n",
    "    # Group the data into m4, m5, kaggle and one group for the rest\n",
    "    grouped_data = {}\n",
    "    other_sum = 0\n",
    "    \n",
    "    for dataset, value in mean_shares.items():\n",
    "        if dataset in ['m4', 'm5', 'kaggle']:\n",
    "            grouped_data[dataset] = value\n",
    "        else:\n",
    "            other_sum += value\n",
    "    \n",
    "    if other_sum > 0:\n",
    "        grouped_data['others'] = other_sum\n",
    "    \n",
    "    grouped_series = pd.Series(grouped_data)\n",
    "    \n",
    "    color_map = {\n",
    "    'others': '#cc0000',  \n",
    "    'kaggle': '#3366cc',  \n",
    "    'm4': '#ff9900',      \n",
    "    'm5': '#339933'       \n",
    "}\n",
    "\n",
    "    colors = []\n",
    "    for label in grouped_series.index:\n",
    "        if label in color_map:\n",
    "            colors.append(color_map[label])\n",
    "        else:\n",
    "            colors.append(None)\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        grouped_series.values, \n",
    "        labels=grouped_series.index,\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90,\n",
    "        pctdistance=0.75,\n",
    "        labeldistance=1.1,\n",
    "        colors=colors,\n",
    "        textprops={'fontsize': 18}\n",
    "    )\n",
    "\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "        autotext.set_fontsize(16)\n",
    "    \n",
    "    for text in texts:\n",
    "        text.set_fontsize(18)\n",
    "    \n",
    "    ax.set_title(f'p={p}', fontsize=18, pad=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{config.path_to_evaluation}/target_shares_div10.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d54a8a3",
   "metadata": {},
   "source": [
    "Data shares of the source datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d19a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average data share for the source data sets of each div over runs\n",
    "results_list = []\n",
    "\n",
    "for div in [\"10\", \"5\", \"0\"]:\n",
    "    dataframes = []\n",
    "    for run in [\"case1_run1\", \"case1_run2\", \"case1_run3\", \"case1_run4\", \"case1_run5\"]:\n",
    "        source = pd.read_parquet(f\"{config.path_to_data}/{run}/interim/source_div{div}{run}/source_div{div}{run}.parquet\")\n",
    "        shares = source.drop_duplicates(subset=\"id_ts\").groupby(by=\"data\").count()[\"id_ts\"]\n",
    "        dataframes.append(shares)\n",
    "    \n",
    "    shares_over_runs = pd.concat(dataframes, axis=1)\n",
    "    mean_shares = shares_over_runs.mean(axis=1)\n",
    "    std_shares = shares_over_runs.std(axis=1)\n",
    "    \n",
    "    result = pd.DataFrame({\n",
    "        'data': mean_shares.index,\n",
    "        f'div={div}': [f\"{mean_val:.2f} ({std_val:.2f})\" for mean_val, std_val in zip(mean_shares.values, std_shares.values)]\n",
    "    })\n",
    "    \n",
    "    results_list.append(result)\n",
    "\n",
    "final_result = results_list[0]\n",
    "for i in range(1, len(results_list)):\n",
    "    final_result = pd.merge(final_result, results_list[i], on='data', how='outer')\n",
    "\n",
    "final_result = final_result.sort_values('data').reset_index(drop=True)\n",
    "final_result.to_csv(f\"{config.path_to_evaluation}/source_data_shares.csv\")\n",
    "print(final_result.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c317fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average source data shares over runs for one div\n",
    "div = \"10\"\n",
    "dataframes = []\n",
    "for run in [\"case1_run1\", \"case1_run2\", \"case1_run3\", \"case1_run4\", \"case1_run5\"]:\n",
    "    source = pd.read_parquet(f\"{config.path_to_data}/{run}/interim/source_div{div}{run}/source_div{div}{run}.parquet\")\n",
    "    shares = source.drop_duplicates(subset=\"id_ts\").groupby(by=\"data\").count()[\"id_ts\"]\n",
    "    dataframes.append(shares)\n",
    "\n",
    "shares_over_runs = pd.concat(dataframes, axis=1)\n",
    "mean_shares = shares_over_runs.mean(axis=1)\n",
    "\n",
    "# Group the data into m4, m5, kaggle and one group for the rest\n",
    "grouped_data = {}\n",
    "other_sum = 0\n",
    "\n",
    "for dataset, value in mean_shares.items():\n",
    "    if dataset in ['m4', 'm5', 'kaggle']:\n",
    "        grouped_data[dataset] = value\n",
    "    else:\n",
    "        other_sum += value\n",
    "\n",
    "if other_sum > 0:\n",
    "    grouped_data['others'] = other_sum\n",
    "\n",
    "grouped_series = pd.Series(grouped_data)\n",
    "\n",
    "\n",
    "color_map = {\n",
    "    'others': '#cc0000',  \n",
    "    'kaggle': '#3366cc',  \n",
    "    'm4': '#ff9900',      \n",
    "    'm5': '#339933'       \n",
    "}\n",
    "\n",
    "\n",
    "colors = []\n",
    "for label in grouped_series.index:\n",
    "    if label in color_map:\n",
    "        colors.append(color_map[label])\n",
    "    else:\n",
    "        colors.append(None)  \n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "\n",
    "wedges, texts, autotexts = plt.pie(\n",
    "    grouped_series.values, \n",
    "    labels=grouped_series.index,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90,\n",
    "    pctdistance=0.75,      \n",
    "    labeldistance=1.1,     \n",
    "    colors=colors,\n",
    "    textprops={'fontsize': 18}\n",
    ")\n",
    "\n",
    "\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "    autotext.set_fontsize(16)\n",
    "\n",
    "\n",
    "for text in texts:\n",
    "    text.set_fontsize(18)\n",
    "    text.set_horizontalalignment('center')\n",
    "\n",
    "plt.axis('equal')  \n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{config.path_to_evaluation}/source_shares_div10.pdf\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main-env-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
